<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" href="../style.css" type="text/css">
<script src="guidance.js"></script>
<title>6.5840 Lab 2: Raft</title>
</head>
<body>
<div align="center">
<h2><a href="../index.html">6.5840</a> - Spring 2023</h2>
<h1>6.5840 Lab 2: Raft</h1>
<h3>Part 2A Due: Friday Feb 24 23:59</h3>
<h3>Part 2B Due: Friday Mar  3 23:59</h3>
<h3>Part 2C Due: Friday Mar 10 23:59</h3>
<h3>Part 2D Due: Friday Mar 17 23:59</h3>


<p>
  <b><a href="collab.html">Collaboration policy</a></b> //
  <b><a href="submit.html">Submit lab</a></b> //
  <b><a href="go.html">Setup Go</a></b> //
  <b><a href="guidance.html">Guidance</a></b> //
  <b><a href="https://piazza.com/mit/spring2023/65840">Piazza</a></b>
</p>

</div>


<hr>

<h3>Introduction</h3>

<p>
This is the first in a series of labs in which you'll build a
fault-tolerant key/value storage system. In this
lab you'll implement Raft, a replicated state machine protocol.
In the next lab you'll build a key/value service on top of
Raft. Then you will &ldquo;shard&rdquo; your service over
multiple replicated state machines for higher performance.

<p>
A replicated service achieves fault
tolerance by storing complete copies of its state (i.e., data)
on multiple replica servers.
Replication allows
the service to continue operating even if some of
its servers experience failures (crashes or a broken or flaky
network). The challenge is that failures may cause the
replicas to hold differing copies of the data.

<p>
Raft organizes client requests into a sequence, called
the log, and ensures that all the replica servers see the same log.
Each replica executes client requests
in log order, applying them to its local copy of the service's state.
Since all the live replicas
see the same log contents, they all execute the same requests
in the same order, and thus continue to have identical service
state. If a server fails but later recovers, Raft takes care of
bringing its log up to date. Raft will continue to operate as
long as at least a majority of the servers are alive and can
talk to each other. If there is no such majority, Raft will
make no progress, but will pick up where it left off as soon as
a majority can communicate again.

<p>
In this lab you'll implement Raft as a Go object type
with associated methods, meant to be used as a module in a
larger service. A set of Raft instances talk to each other with
RPC to maintain replicated logs. Your Raft interface will
support an indefinite sequence of numbered commands, also
called log entries. The entries are numbered with <em>index
numbers</em>. The log entry with a given index will eventually
be committed. At that point, your Raft should send the log
entry to the larger service for it to execute.

<p>
You should follow the design in the
<a href="../papers/raft-extended.pdf">extended Raft paper</a>,
with particular attention to Figure 2.
You'll implement most of what's in the paper, including saving
persistent state and reading it after a node fails and
then restarts. You will not implement cluster
membership changes (Section 6).

<p>
You may find this
<a href="https://thesquareplanet.com/blog/students-guide-to-raft/">guide</a>
useful, as well as this advice about
<a href="raft-locking.txt">locking</a>
and
<a href="raft-structure.txt">structure</a>
for concurrency.
For a
wider perspective, have a look at Paxos, Chubby, Paxos Made
Live, Spanner, Zookeeper, Harp, Viewstamped Replication, and
<a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf">Bolosky et al.</a>
(Note: the student's guide was written several years ago, and part 2D in particular has
since changed. Make sure you understand why a particular implementation strategy makes
sense before blindly following it!)

<p>
Keep in mind that the most challenging part of this lab may not be implementing your solution, but debugging it.
To help address this challenge, you may wish to spend time thinking about how to make your implementation more easily debuggable. You might refer to the <a href="guidance.html">Guidance</a> page and to <a href="https://blog.josejg.com/debugging-pretty/">this blog post about effective print statements</a>.

<p>
We also provide a <a href="../notes/raft_diagram.pdf">diagram of Raft interactions</a> that can
help clarify how your Raft code interacts with the layers on top of it.

<p>
This lab is due in four parts. You must submit each part on the
corresponding due date.

<h3>Getting Started</h3>

<p>
If you have done Lab 1, you already have a copy of the lab
source code.
If not,
you can find directions for obtaining the source via git
in the <a href="lab-mr.html">Lab 1 instructions</a>.

<p>
We supply you with skeleton code <tt>src/raft/raft.go</tt>. We also
supply a set of tests, which you should use to drive your
implementation efforts, and which we'll use to grade your submitted
lab. The tests are in <tt>src/raft/test_test.go</tt>.

<p>
When we grade your submissions, we will run the tests without the <a href="https://go.dev/blog/race-detector"><tt>-race</tt> flag</a>.
However, you should check that your code does not have races,
by running
the tests with the <tt>-race</tt> flag as you develop your solution.

<p>
To get up and running, execute the following commands.
Don't forget the <tt>git pull</tt> to get the latest software.
<pre>
$ cd ~/6.5840
$ git pull
...
$ cd src/raft
$ go test
Test (2A): initial election ...
--- FAIL: TestInitialElection2A (5.04s)
        config.go:326: expected one leader, got none
Test (2A): election after network failure ...
--- FAIL: TestReElection2A (5.03s)
        config.go:326: expected one leader, got none
...
$
</pre>

<h3>The code</h3>

Implement Raft by adding code to
<tt>raft/raft.go</tt>. In that file you'll find
skeleton code, plus examples of how to send and receive
RPCs.

<p>
Your implementation must support the following interface, which
the tester and (eventually) your key/value server will use.
You'll find more details in comments in <tt>raft.go</tt>.

<pre>
// create a new Raft server instance:
rf := Make(peers, me, persister, applyCh)

// start agreement on a new log entry:
rf.Start(command interface{}) (index, term, isleader)

// ask a Raft for its current term, and whether it thinks it is leader
rf.GetState() (term, isLeader)

// each time a new entry is committed to the log, each Raft peer
// should send an ApplyMsg to the service (or tester).
type ApplyMsg</pre>

<p>
A service calls <tt>Make(peers,me,&hellip;)</tt> to create a
Raft peer. The peers argument is an array of network identifiers
of the Raft peers (including this one), for use with RPC. The
<tt>me</tt> argument is the index of this peer in the peers
array. <tt>Start(command)</tt> asks Raft to start the processing
to append the command to the replicated log. <tt>Start()</tt>
should return immediately, without waiting for the log appends
to complete. The service expects your implementation to send an
<tt>ApplyMsg</tt> for each newly committed log entry to the
<tt>applyCh</tt> channel argument to <tt>Make()</tt>.

<p>
<tt>raft.go</tt> contains example code that sends an RPC
(<tt>sendRequestVote()</tt>) and that handles an incoming RPC
(<tt>RequestVote()</tt>).
Your Raft peers should exchange RPCs using the labrpc Go
package (source in <tt>src/labrpc</tt>).
The tester can tell <tt>labrpc</tt> to delay RPCs,
re-order them, and discard them to simulate various network failures.
While you can temporarily modify <tt>labrpc</tt>,
make sure your Raft works with the original <tt>labrpc</tt>,
since that's what we'll use to test and grade your lab.
Your Raft instances must interact only with RPC; for example,
they are not allowed to communicate using shared Go variables
or files.

<p>
Subsequent labs build on this lab, so it is important to give
yourself enough time to write solid code.

<h3>Part 2A: leader election <script>g("moderate")</script></h3>

<p class="todo">
Implement Raft leader election and heartbeats (<tt>AppendEntries</tt> RPCs with no
log entries). The goal for Part 2A is for a
single leader to be elected, for the leader to remain the leader
if there are no failures, and for a new leader to take over if the
old leader fails or if packets to/from the old leader are lost.
Run <tt>go test -run 2A </tt> to test your 2A code.

<ul class="hints">

<li>You can't easily run your Raft implementation directly; instead you should
run it by way of the tester, i.e. <tt>go test -run 2A </tt>.

<li>Follow the paper's Figure 2. At this point you care about sending
and receiving RequestVote RPCs, the Rules for Servers that relate to
elections, and the State related to leader election,

<li>
Add the Figure 2 state for leader election
to the <tt>Raft</tt> struct in <tt>raft.go</tt>.
You'll also need to define a
struct to hold information about each log entry.

<li>
Fill in the <tt>RequestVoteArgs</tt> and
<tt>RequestVoteReply</tt> structs. Modify
<tt>Make()</tt> to create a background goroutine that will kick off leader
 election periodically by sending out <tt>RequestVote</tt> RPCs when it hasn't
 heard from another peer for a while.  This way a peer will learn who is the
 leader, if there is already a leader, or become the leader itself.  Implement
 the <tt>RequestVote()</tt> RPC handler so that servers will vote for one
 another.


<li>
To implement heartbeats, define an
<tt>AppendEntries</tt> RPC struct (though you may not
need all the arguments yet), and have the leader send
them out periodically. Write an
<tt>AppendEntries</tt> RPC handler method that resets
the election timeout so that other servers don't step
forward as leaders when one has already been elected.

<li>
Make sure the election timeouts in different peers don't always fire
at the same time, or else all peers will vote only for themselves and no
one will become the leader.

<li>
The tester requires that the leader send heartbeat RPCs no more than
ten times per second.

<li>
The tester requires your Raft to elect a new leader within five seconds of the
failure of the old leader (if a majority of peers can still
communicate). Remember, however, that leader election may require multiple
rounds in case of a split vote (which can happen if packets are lost or if
candidates unluckily choose the same random backoff times). You must pick
election timeouts (and thus heartbeat intervals) that are short enough that it's
very likely that an election will complete in less than five seconds even if it
requires multiple rounds.

<li>
The paper's Section 5.2 mentions election timeouts in the range of 150
to 300 milliseconds. Such a range only makes sense if the leader
sends heartbeats considerably more often than once per 150
milliseconds (e.g., once per 10 milliseconds). Because the tester limits you tens of heartbeats per
second, you will have to use an election timeout larger
than the paper's 150 to 300 milliseconds, but not too large, because then you
may fail to elect a leader within five seconds.

<li>
You may find Go's
<a href="https://golang.org/pkg/math/rand/">rand</a>
useful.

<li>
You'll need to write code that takes actions periodically or
after delays in time. The easiest way to do this is to create
a goroutine with a loop that calls
<a href="https://golang.org/pkg/time/#Sleep">time.Sleep()</a>;
see the <tt>ticker()</tt> goroutine that <tt>Make()</tt>
creates for this purpose.
Don't use Go's <tt>time.Timer</tt> or <tt>time.Ticker</tt>, which
are difficult to use correctly.

<li>The <a href="guidance.html">Guidance page</a> has some
  tips on how to develop and debug your code.

<li>
If your code has trouble passing the tests,
read the paper's Figure 2 again; the full logic for leader
election is spread over multiple parts of the figure.

<li>
Don't forget to implement <tt>GetState()</tt>.

<li>
The tester calls your Raft's <tt>rf.Kill()</tt> when it is
permanently shutting down an instance. You can check whether
<tt>Kill()</tt> has been called using <tt>rf.killed()</tt>.
You may want to do this in all loops, to avoid having
dead Raft instances print confusing messages.


<li>Go RPC sends only struct fields whose names start with capital letters.
  Sub-structures must also have capitalized field names (e.g. fields of log records
  in an array). The <tt>labgob</tt> package will warn you about this;
  don't ignore the warnings.

</ul>


<p>
Be sure you pass the 2A tests before submitting Part 2A, so that
you see something like this:

<pre>
$ go test -run 2A
Test (2A): initial election ...
  ... Passed --   3.5  3   58   16840    0
Test (2A): election after network failure ...
  ... Passed --   5.4  3  118   25269    0
Test (2A): multiple elections ...
  ... Passed --   7.3  7  624  138014    0
PASS
ok  	6.5840/raft	16.265s
$
</pre>

<p>
Each "Passed" line contains five numbers; these are the time that the
test took in seconds, the number of Raft peers, the
number of RPCs sent during the test, the total number of bytes in the
RPC messages, and the number of log entries
that Raft reports were committed. Your numbers will differ from those
shown here. You can ignore the numbers if you like, but they may help
you sanity-check the number of RPCs that your implementation sends.
For all of labs 2, 3, and 4, the grading script will fail your
solution if it takes more than 600 seconds for all of the tests
(<tt>go test</tt>), or if any individual test takes more than 120
seconds.

<p>
When we grade your submissions, we will run the tests without the <a href="https://go.dev/blog/race-detector"><tt>-race</tt> flag</a>. However, you should make sure that your code
consistently passes the tests with the <tt>-race</tt> flag.

<h3>Part 2B: log <script>g("hard")</script></h3>

<p class="todo">
Implement the leader and follower code to append new log entries,
so that the <tt>go test -run 2B </tt> tests pass.

<ul class="hints">

<li>
Run <tt>git pull</tt> to get the latest lab software.

<li>
Your first goal should be to pass <tt>TestBasicAgree2B()</tt>.
Start by implementing <tt>Start()</tt>, then write the code
to send and receive new log entries via <tt>AppendEntries</tt> RPCs,
following Figure 2. Send each newly committed entry
on <tt>applyCh</tt> on each peer.

<li>
You will need to implement the election
restriction (section 5.4.1 in the paper).

<li>
One way to fail to reach agreement in the early Lab 2B
tests is to hold repeated elections even though the
leader is alive. Look for bugs in election timer
management, or not sending out heartbeats immediately after winning an
election.

<li>
Your code may have loops that repeatedly check for certain events.
Don't have these loops
execute continuously without pausing, since that
will slow your implementation enough that it fails tests.
Use Go's
<a href="https://golang.org/pkg/sync/#Cond">condition variables</a>,
or insert a
<tt>time.Sleep(10 * time.Millisecond)</tt> in each loop iteration.

<li>Do yourself a favor for future labs and write (or re-write) code
that's clean and clear.  For ideas, re-visit our
the <a href="guidance.html">Guidance page</a> with tips on how to
  develop and debug your code.

<li>If you fail a test, look over the code for the test
  in <tt>config.go</tt> and <tt>test_test.go</tt> to get a better
  understanding what the test is testing.  <tt>config.go</tt> also
  illustrates how the tester uses the Raft API.

</ul>

<p>
The tests for upcoming labs may fail your code if it runs too slowly.
You can check how much real time and CPU time your solution uses with
the time command. Here's typical output:

<pre>
$ time go test -run 2B
Test (2B): basic agreement ...
  ... Passed --   0.9  3   16    4572    3
Test (2B): RPC byte count ...
  ... Passed --   1.7  3   48  114536   11
Test (2B): agreement after follower reconnects ...
  ... Passed --   3.6  3   78   22131    7
Test (2B): no agreement if too many followers disconnect ...
  ... Passed --   3.8  5  172   40935    3
Test (2B): concurrent Start()s ...
  ... Passed --   1.1  3   24    7379    6
Test (2B): rejoin of partitioned leader ...
  ... Passed --   5.1  3  152   37021    4
Test (2B): leader backs up quickly over incorrect follower logs ...
  ... Passed --  17.2  5 2080 1587388  102
Test (2B): RPC counts aren't too high ...
  ... Passed --   2.2  3   60   20119   12
PASS
ok  	6.5840/raft	35.557s

real	0m35.899s
user	0m2.556s
sys	0m1.458s
$
</pre>

The "ok 6.5840/raft 35.557s" means that Go measured the time taken for the 2B
tests to be 35.557 seconds of real (wall-clock) time. The "user
0m2.556s" means that the code consumed 2.556 seconds of CPU time, or
time spent actually executing instructions (rather than waiting or
sleeping). If your solution uses much more than a minute of real time
for the 2B tests, or much more than 5 seconds of CPU time, you may run
into trouble later on. Look for time spent sleeping or waiting for RPC
timeouts, loops that run without sleeping or waiting for conditions or
channel messages, or large numbers of RPCs sent.

<h3>Part 2C: persistence <script>g("hard")</script></h3>

<p>
If a Raft-based server reboots it should resume service
where it left off. This requires
that Raft keep persistent state that survives a reboot. The
paper's Figure 2 mentions which state should be persistent.

<p>
A real implementation would write
Raft's persistent state to disk each time it changed, and would read the
state from
disk when restarting after a reboot. Your implementation won't use
the disk; instead, it will save and restore persistent state
from a <tt>Persister</tt> object (see <tt>persister.go</tt>).
Whoever calls <tt>Raft.Make()</tt> supplies a <tt>Persister</tt>
that initially holds Raft's most recently persisted state (if
any). Raft should initialize its state from that
<tt>Persister</tt>, and should use it to save its persistent
state each time the state changes. Use the <tt>Persister</tt>'s
<tt>ReadRaftState()</tt> and <tt>Save()</tt> methods.

<p class="todo">
Complete the functions
<tt>persist()</tt>
and
<tt>readPersist()</tt> in <tt>raft.go</tt>
by adding code to save and restore persistent state. You will need to encode
(or "serialize") the state as an array of bytes in order to pass it to
the <tt>Persister</tt>. Use the <tt>labgob</tt> encoder;
see the comments in <tt>persist()</tt> and <tt>readPersist()</tt>.
<tt>labgob</tt> is like Go's <tt>gob</tt> encoder but
prints error messages if
you try to encode structures with lower-case field names.
For now, pass <tt>nil</tt> as the second argument to <tt>persister.Save()</tt>.
Insert calls to <tt>persist()</tt> at the points where
your implementation changes persistent state.
Once you've done this,
and if the rest of your implementation is correct,
you should pass all of the 2C tests.

<p>
You will probably need the optimization that backs up
nextIndex by more than one entry
at a time. Look at the <a href="../papers/raft-extended.pdf">extended Raft paper</a> starting at
the bottom of page 7 and top of page 8 (marked by a gray line).
The paper is vague about the details; you will need to fill in the gaps.
One possibility is to have a rejection message include:

<pre>
    XTerm:  term in the conflicting entry (if any)
    XIndex: index of first entry with that term (if any)
    XLen:   log length
</pre>

Then the leader's logic can be something like:

<pre>
  Case 1: leader doesn't have XTerm:
    nextIndex = XIndex
  Case 2: leader has XTerm:
    nextIndex = leader's last entry for XTerm
  Case 3: follower's log is too short:
    nextIndex = XLen
</pre>

A few other hints:

<ul class="hints">

<li>
Run <tt>git pull</tt> to get the latest lab software.

<li>
The 2C tests are more demanding than those for 2A or 2B, and failures
may be caused by problems in your code for 2A or 2B.

</ul>

<p>
Your code should pass all the 2C tests (as shown below), as well as
the 2A and 2B tests.

<pre>
$ go test -run 2C
Test (2C): basic persistence ...
  ... Passed --   5.0  3   86   22849    6
Test (2C): more persistence ...
  ... Passed --  17.6  5  952  218854   16
Test (2C): partitioned leader and one follower crash, leader restarts ...
  ... Passed --   2.0  3   34    8937    4
Test (2C): Figure 8 ...
  ... Passed --  31.2  5  580  130675   32
Test (2C): unreliable agreement ...
  ... Passed --   1.7  5 1044  366392  246
Test (2C): Figure 8 (unreliable) ...
  ... Passed --  33.6  5 10700 33695245  308
Test (2C): churn ...
  ... Passed --  16.1  5 8864 44771259 1544
Test (2C): unreliable churn ...
  ... Passed --  16.5  5 4220 6414632  906
PASS
ok  	6.5840/raft	123.564s
$
</pre>

<p>It is a good idea to run the tests multiple times before
  submitting and check that each run prints <tt>PASS</tt>.
<pre>
$ for i in {0..10}; do go test; done
</pre>


<h3>Part 2D: log compaction <script>g("hard")</script></h3>

<p>
As things stand now, a rebooting server replays the
complete Raft log in order to restore its state. However, it's not
practical for a long-running service to remember the complete Raft log
forever. Instead, you'll modify Raft to cooperate with services that
persistently store a "snapshot" of their state from time to time, at
which point Raft discards log entries that precede the snapshot. The
result is a smaller amount of persistent data and faster restart.
However, it's now possible for a follower to fall so far behind that
the leader has discarded the log entries it needs to catch up; the
leader must then send a snapshot plus the log starting at the time of
the snapshot. Section 7 of the
<a href="../papers/raft-extended.pdf">extended Raft paper</a>
outlines the scheme; you will have to design the details.

<p>
You may find it helpful to refer to the <a href="../notes/raft_diagram.pdf">diagram of Raft
    interactions</a> to understand how the replicated service and Raft communicate.

<p>
Your Raft must provide the following function that the service
can call with a serialized snapshot of its state:

<p><tt>Snapshot(index int, snapshot []byte)</tt>

<p>
In Lab 2D, the tester calls <tt>Snapshot()</tt> periodically. In Lab 3, you will
write a key/value server that calls <tt>Snapshot()</tt>; the snapshot
will contain the complete table of key/value pairs.
The service layer calls <tt>Snapshot()</tt> on every peer (not
just on the leader).

<p>The <tt>index</tt> argument indicates the highest log entry that's
reflected in the snapshot. Raft should discard its log entries before
that point. You'll need to revise your Raft code to operate while
storing only the tail of the log.

<p>
You'll need to implement the <tt>InstallSnapshot</tt> RPC discussed in
the paper that allows a Raft leader to tell a lagging Raft peer to
replace its state with a snapshot. You will likely need to think
through how InstallSnapshot should interact with the state and rules
in Figure 2.

<p>
When a follower's Raft code receives an InstallSnapshot RPC, it can
use the <tt>applyCh</tt> to send the snapshot to the service in
an <tt>ApplyMsg</tt>. The <tt>ApplyMsg</tt> struct definition already
contains the fields you will need (and which the tester expects). Take
care that these snapshots only advance the service's state, and don't
cause it to move backwards.

<p>
If a server crashes, it must restart from persisted data. Your Raft
should persist both Raft state and the corresponding snapshot.
Use the second argument to
<tt>persister.Save()</tt> to save the snapshot.
If there's no snapshot, pass <tt>nil</tt> as the second
argument.

<p>
When a server restarts, the application layer reads the persisted
snapshot and restores its saved state.

<p class="todo">
Implement <tt>Snapshot()</tt> and the InstallSnapshot RPC, as well as the
changes to Raft to support these (e.g, operation with a
trimmed log).  Your solution is complete when it passes the 2D tests
(and all the previous Lab 2 tests).
</p>

<ul class="hints">

<li> <tt>git pull</tt> to make sure you have the latest software.

<li> A good place to start is to modify your code to so that it is
able to store just the part of the log
starting at some index X. Initially you can set X to zero and
run the 2B/2C tests.
Then make <tt>Snapshot(index)</tt> discard the log before <tt>index</tt>,
and set X equal to <tt>index</tt>. If all goes well you should
now pass the first 2D test.

<li> You won't be able to store the log in a Go slice and use Go slice
indices interchangeably with Raft log indices; you'll need to index
the slice in a way that accounts for the discarded portion of the log.

<li>Next: have the leader send an InstallSnapshot RPC if it doesn't
have the log entries required to bring a follower up to date.

<li>Send the entire snapshot in a single InstallSnapshot RPC.
Don't implement Figure 13's <tt>offset</tt> mechanism for
splitting up the snapshot.

<li> Raft must discard old log entries in a way that allows the Go garbage collector to free and re-use the
memory; this requires that there be no reachable references (pointers)
to the discarded log entries.


    <li> Even when the log is trimmed, your implemention still needs to properly send the term and index
    of the entry prior to new entries in <tt>AppendEntries</tt> RPCs; this may require saving and referencing
    the latest snapshot's <tt>lastIncludedTerm/lastIncludedIndex</tt> (consider whether this
    should be persisted).

<li>
A reasonable amount of time to consume for the full set of
Lab 2 tests (2A+2B+2C+2D) without <tt>-race</tt> is 6 minutes of real time and one
minute of CPU time. When running with <tt>-race</tt>, it is about 10 minutes of real
time and two minutes of CPU time.

</ul>

<p>
Your code should pass all the 2D tests (as shown below), as well as the 2A, 2B, and 2C tests.

<pre>
$ go test -run 2D
Test (2D): snapshots basic ...
  ... Passed --  11.6  3  176   61716  192
Test (2D): install snapshots (disconnect) ...
  ... Passed --  64.2  3  878  320610  336
Test (2D): install snapshots (disconnect+unreliable) ...
  ... Passed --  81.1  3 1059  375850  341
Test (2D): install snapshots (crash) ...
  ... Passed --  53.5  3  601  256638  339
Test (2D): install snapshots (unreliable+crash) ...
  ... Passed --  63.5  3  687  288294  336
Test (2D): crash and restart all servers ...
  ... Passed --  19.5  3  268   81352   58
PASS
ok      6.5840/raft      293.456s
</pre>

</body>
</html>
<!--  LocalWords:  transactional RPC snapshotting Paxos Viewstamped -->
<!--  LocalWords:  Bolosky et al else's github src labrpc cd ok RPCs -->
<!--  LocalWords:  TestInitialElection TestReElection rf persister -->
<!--  LocalWords:  applyCh isleader GetState isLeader rpc -->
<!--  LocalWords:  ApplyMsg Persister's ReadRaftState -->
<!--  LocalWords:  readPersist sendRequestVote RequestVote struct -->
<!--  LocalWords:  RequestVoteArgs RequestVoteReply structs goroutine -->
<!--  LocalWords:  AppendEntries AppendEntry TestBasicAgree -->
<!--  LocalWords:  InstallSnapshot -->
